diff --git a/data_processing/wind_speed_dataloader.py b/data_processing/wind_speed_dataloader.py
index 533b7fa..53deb27 100644
--- a/data_processing/wind_speed_dataloader.py
+++ b/data_processing/wind_speed_dataloader.py
@@ -19,7 +19,7 @@ warnings.filterwarnings('ignore')
 class Dataset_wind(Dataset):
     def __init__(self, root_path, flag='train', size=None, 
                  features='S', data_path='Wind_speed_data.csv', 
-                 target='50m', scale=True, inverse=False, timeenc=0, freq='h', cols=None):
+                 target='80m', scale=True, inverse=False, timeenc=0, freq='h', cols=None):
         #size [seq_len, label_len, pred_len]
         # info
         if size == None:
@@ -148,7 +148,7 @@ class Dataset_wind(Dataset):
 class Dataset_Pred(Dataset):
     def __init__(self, root_path, flag='pred', size=None, 
                  features='S', data_path='Wind_speed_data.csv', 
-                 target='50m', scale=True, inverse=False, timeenc=0, freq='15min', cols=None):
+                 target='80m', scale=True, inverse=False, timeenc=0, freq='7min', cols=None):
         #size [seq_len, label_len, pred_len]
         # info
         if size == None:
diff --git a/experiments/exp_wind.py b/experiments/exp_wind.py
index d15f9c5..023e46e 100644
--- a/experiments/exp_wind.py
+++ b/experiments/exp_wind.py
@@ -76,11 +76,10 @@ class Exp_wind(Exp_Basic):
             freq=freq,
             cols=args.cols
         )
-        dataset_json_str = json.dumps(list(data_set))
+        length_data = len(data_set)
+        print(flag,length_data )
         wandb.log({
-            flag + "_Dataset": dataset_json_str})
-        print(flag, len(data_set))
-        
+            flag + "_Dataset": length_data})
         data_loader = DataLoader(
             data_set,
             batch_size=batch_size,
@@ -92,8 +91,6 @@ class Exp_wind(Exp_Basic):
 
     def _select_optimizer(self):
         model_optim = optim.Adam(self.model.parameters(), lr=self.args.lr)
-        wandb.log({
-            "Optimizer": model_optim })
         return model_optim
     
     def _select_criterion(self, losstype):
@@ -252,9 +249,6 @@ class Exp_wind(Exp_Basic):
             writer.add_scalar('train_loss', train_loss, global_step=epoch)
             writer.add_scalar('valid_loss', valid_loss, global_step=epoch)
             writer.add_scalar('test_loss', test_loss, global_step=epoch)
-            wandb.log({"Speed": speed})
-            wandb.log({"left_time ": left_time })
-            wandb.log({"loss": loss})
             wandb.log({"Train Loss": train_loss})
             wandb.log({"Validation Loss": valid_loss})
             wandb.log({"Test Loss": test_loss})
@@ -266,8 +260,7 @@ class Exp_wind(Exp_Basic):
                 break
 
             lr = adjust_learning_rate(model_optim, epoch+1, self.args)
-            wandb.log({"Learning rate adjustent": lr})
-            
+            wandb.log({"Learning rate adjustent": lr}) 
         save_model(epoch, lr, self.model, path, model_name=self.args.data, horizon=self.args.pred_len)
         best_model_path = path+'/'+'checkpoint.pth'
         self.model.load_state_dict(torch.load(best_model_path))
diff --git a/train_wind.py b/train_wind.py
index ecd937e..304049f 100644
--- a/train_wind.py
+++ b/train_wind.py
@@ -15,7 +15,7 @@ parser.add_argument('--data', type=str, required=False, default='Wind_speed_data
 parser.add_argument('--root_path', type=str, default='./dataset/', help='root path of the data file')
 parser.add_argument('--data_path', type=str, default='Wind_speed_data', help='location of the data file')
 parser.add_argument('--features', type=str, default='S', choices=['S', 'M'], help='features S is univariate, M is multivariate')
-parser.add_argument('--target', type=str, default='60m', help='target feature')
+parser.add_argument('--target', type=str, default='80m', help='target feature')
 parser.add_argument('--freq', type=str, default='h', help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')
 parser.add_argument('--checkpoints', type=str, default='exp/wind_checkpoints/', help='location of model checkpoints')
 parser.add_argument('--inverse', type=bool, default =False, help='denorm the output data')
@@ -31,7 +31,7 @@ parser.add_argument('--devices', type=str, default='0',help='device ids of multi
 ### -------  input/output length settings --------------                                                                            
 parser.add_argument('--seq_len', type=int, default=32, help='input sequence length of resnet')
 parser.add_argument('--label_len', type=int, default=32, help='start token length of Informer decoder')
-parser.add_argument('--pred_len', type=int, default=64, help='prediction sequence length, horizon')
+parser.add_argument('--pred_len', type=int, default=128, help='prediction sequence length, horizon')
 parser.add_argument('--concat_len', type=int, default=0)
 parser.add_argument('--single_step', type=int, default=0)
 parser.add_argument('--single_step_output_One', type=int, default=0)
@@ -42,7 +42,7 @@ parser.add_argument('--cols', type=str, nargs='+', help='file list')
 parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')
 parser.add_argument('--itr', type=int, default=0, help='experiments times')
 parser.add_argument('--train_epochs', type=int, default=100, help='train epochs')
-parser.add_argument('--batch_size', type=int, default=64, help='batch size of train input data')
+parser.add_argument('--batch_size', type=int, default=18, help='batch size of train input data')
 parser.add_argument('--patience', type=int, default=10, help='early stopping patience')
 parser.add_argument('--lr', type=float, default=0.0001, help='optimizer learning rate')
 parser.add_argument('--loss', type=str, default='RMSE',help='loss function')
@@ -61,7 +61,7 @@ parser.add_argument('--base_filter', type=int, default=4, help='number of filter
 parser.add_argument('--groups', default=1, type=int, help='set largest to 1')
 parser.add_argument('--n_block', default=4, type=int, help='number of blocks')
 parser.add_argument('--kernel', default=3, type=int, help='width of kernel')
-parser.add_argument('--n_classes', default=64, type=int, help='number of classes')
+parser.add_argument('--n_classes', default=128, type=int, help='number of classes')
 
 
 
